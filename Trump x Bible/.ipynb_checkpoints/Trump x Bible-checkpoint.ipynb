{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Trump tweets and Bible Verses in a Markov Model and generate random text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import normalize\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Old Testament of the King James Version of the Bible\n",
      "\n",
      "\n",
      "\n",
      " The First Book of Moses:  Called Genesis\n",
      "\n",
      " 1:1 In the beginning God created the heavens and the earth.\n",
      " 1:2 And the earth was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters.\n",
      " 1:3 And God said, Let there be light: and there was light.\n",
      " 1:4 And God saw the light, that it was good: and God divided the light from the darkness.\n",
      " 1:5 And God called the light Day, and the darkness he called Night. And the evening and the morning were the first day.\n",
      " 1:6 And God said, Let there be a firmament in the midst of the waters, and let it divide the waters from the waters.\n",
      " 1:7 And God made the firmament, and divided the waters which were under the firmament from the waters which were above the firmament: and it was so.\n",
      " 1:8 And God called the firmament Heaven. And the evening and the morning were the second day.\n",
      " 1:9 And God said, Let the waters under the heaven be\n"
     ]
    }
   ],
   "source": [
    "# Read in the bible\n",
    "with open(\"bible.txt\", \"r\") as bible:\n",
    "    bible_text = bible.read()\n",
    "    bible.close()\n",
    "    \n",
    "# Regular expression to consolidate verses into single lines\n",
    "single_line = re.compile(\"\\n(?!\\n)\")\n",
    "bible_text = re.sub(pattern=single_line, string=bible_text, repl=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgree\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (15,30,31,32,36,37,38,42,43,44,45,46,47,50,51,52,56,57,58,59,60,61,62,63,64,65,66,67) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Read in the trump tweets\n",
    "tweets = pd.read_csv(\"trumpTweets.csv\")\n",
    "tweets_text = tweets[\"text\"]\n",
    "del tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append tweets to Bible\n",
    "bible_and_tweets = bible_text + \"\\n\".join(tweets_text)\n",
    "with open(\"bible_and_tweets.txt\", \"w+\", encoding=\"utf-8\") as file:\n",
    "    file.write(bible_and_tweets)\n",
    "    file.close()\n",
    "del bible_text, bible_and_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back in the results\n",
    "with open(\"bible_and_tweets.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    bible_and_tweets = file.read()\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regular expression to edit data\n",
    "remove_non_alphabetic = re.compile(\"[^a-zA-Z \\n]\")\n",
    "remove_multiple_newlines = re.compile(\"\\n[\\n]+\")\n",
    "remove_multiple_spaces = re.compile(\" [ ]+\")\n",
    "remove_http = re.compile(\"http[^ ]+\")\n",
    "bible_and_tweets = re.sub(pattern=remove_non_alphabetic, string=bible_and_tweets, repl=\"\")\n",
    "bible_and_tweets = re.sub(pattern=remove_multiple_newlines, string=bible_and_tweets, repl=\"\\n\")\n",
    "bible_and_tweets = re.sub(pattern=remove_http, string=bible_and_tweets, repl=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_and_tweets = bible_and_tweets.replace(\"\\n\", \" ENDLINE\\n\")\n",
    "bible_and_tweets = re.sub(pattern=remove_multiple_spaces, string=bible_and_tweets, repl=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the old testament of the king james version of the bible endline\n",
      " the first book of moses called genesis endline\n",
      " in the beginning god created the heavens and the earth endline\n",
      " and the earth was without form and void and darkness was upon the face of the deep and the spirit of god moved upon the face of the waters endline\n",
      " and god said let there be light and there was light endline\n",
      " and god saw the light that it was good and god divided the light from the darkness endline\n",
      " and god called the light day and the darkness he called night and the evening and the morning were the first day endline\n",
      " and god said let there be a firmament in the midst of the waters and let it divide the waters from the waters endline\n",
      " and god made the firmament and divided the waters which were under the firmament from the waters which were above the firmament and it was so endline\n",
      " and god called the firmament heaven and the evening and the morning were the second day endline\n",
      " and god said let the waters unde\n",
      "d trace not penn are you drugged endline\n",
      "dennisrodman apprenticenbc dennisyou were greatthanks endline\n",
      "christinefox see you later endline\n",
      "vanindc dannyzuker thanks vanso true endline\n",
      "jilltoma hi amy see you soon endline\n",
      "shock obama wh given three pinocchios for lying about benghazi emails it amazing that obama never knew about the irs scandals until he saw it in the news endline\n",
      "very sad that republican donors were targeted by obamas irs endline\n",
      "one season ends and another starts already casting for the next apprenticenbc great news for charity million so far endline\n",
      "emtgonenutz thanks tanja endline\n",
      "the lincoln day dinner last night in michigan was fantastic record attendance and tremendous enthusiasm i loved it endline\n",
      " circulation is way down and all he thinks about are his bad food restaurants condenastcorp endline\n",
      "i cant believe vanityfair would renew graydon carters contract endline\n",
      "graydon carter is laughing at the stupidity of chuck townsend on his contract renewal even he doesn\n"
     ]
    }
   ],
   "source": [
    "bible_and_tweets = bible_and_tweets.lower()\n",
    "print(bible_and_tweets[0:1000])\n",
    "print(bible_and_tweets[5000000:5001000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim each line, write result back to file\n",
    "sentences = []\n",
    "for line in bible_and_tweets.split(\"endline\\n\"):\n",
    "    sentences.append(line.strip())\n",
    "\n",
    "with open(\"text_processed.txt\", \"w+\") as file:\n",
    "    file.write(\" endline\\n\".join(sentences))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocabulary and transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7598559\n"
     ]
    }
   ],
   "source": [
    "# Read in the full text\n",
    "with open(\"text_processed.txt\", \"r\") as file:\n",
    "    text_full = file.read()\n",
    "    file.close()\n",
    "print(len(text_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1366285\n"
     ]
    }
   ],
   "source": [
    "# Create the vocabulary\n",
    "vocab = []\n",
    "for line in text_full.split(\"\\n\"):\n",
    "    for word in line.split(\" \"):\n",
    "        vocab.append(word)\n",
    "        \n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43714\n"
     ]
    }
   ],
   "source": [
    "# Create a set for the vocab\n",
    "vocab = set(vocab)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dict mapping vocab to index\n",
    "vocab_to_id = {}\n",
    "id_to_vocab = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    vocab_to_id[word] = index\n",
    "    id_to_vocab[index] = word\n",
    "    \n",
    "# Save the objects\n",
    "with open('vocab_to_id.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab_to_id, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('id_to_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(id_to_vocab, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Create an empty matrix of zeroes\n",
    "tf = np.zeros((len(vocab), len(vocab)))\n",
    "print(type(tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill up the tf matrix\n",
    "for line in text_full.split(\"\\n\"):\n",
    "    words = line.split(\" \")\n",
    "    for i in range(len(words) - 1):\n",
    "        id1 = vocab_to_id[words[i]]\n",
    "        id2 = vocab_to_id[words[i + 1]]\n",
    "        tf[id1, id2] += 1\n",
    "del text_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15287310480\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "print(sys.getsizeof(tf))\n",
    "mat = sp.coo_matrix(tf)\n",
    "print(sys.getsizeof(mat))\n",
    "del tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37577\n",
      "4716\n",
      "7159\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(mat.data))\n",
    "print(mat.row[37577])\n",
    "print(mat.col[37577])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the son\n",
      "son of\n",
      "on the\n",
      "said unto\n",
      "thank you\n",
      "will be\n",
      "of israel\n",
      "out of\n",
      "unto the\n",
      "and they\n",
      "all the\n",
      "shall be\n",
      "for the\n",
      "i will\n",
      "and he\n",
      "to the\n",
      "in the\n",
      "and the\n",
      "the lord\n",
      "of the\n"
     ]
    }
   ],
   "source": [
    "# Find the k most common word pairings\n",
    "k = 20\n",
    "elems = mat.data.copy()\n",
    "elems.sort()\n",
    "for elem in elems[-k:]:\n",
    "    loc = np.where(mat.data == elem)\n",
    "    id1 = mat.row[loc[0][0]]\n",
    "    id2 = mat.col[loc[0][0]]\n",
    "    print(str(id_to_vocab[id1]) + \" \" + id_to_vocab[id2] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common consectuive pair of words, with 13245 instances, was \"of the\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the matrix\n",
    "mat_norm = normalize(mat, norm='l1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the normalized matrix\n",
    "sp.save_npz(\"norm_matrix.npz\", mat_norm)\n",
    "del mat, mat_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually make random sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back in the matrix and dictionary objects\n",
    "mat_norm = sp.load_npz(\"norm_matrix.npz\")\n",
    "with open('vocab_to_id.pkl', 'rb') as f:\n",
    "    pickle.load(f)\n",
    "with open('id_to_vocab.pkl', 'rb') as f:\n",
    "    pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who was a son of them thou shalt thou art not the word again despite obamas terrible endline\n"
     ]
    }
   ],
   "source": [
    "start_word = \"who\"\n",
    "if start_word not in vocab:\n",
    "    print(\"Invalid start word\")\n",
    "\n",
    "sentence = start_word\n",
    "while start_word != \"endline\":\n",
    "    row_ind = vocab_to_id[start_word]\n",
    "    prob_dist = np.array(mat_norm.getrow(row_ind).todense())[0]\n",
    "    next_ind = np.random.choice(range(len(vocab)), p=prob_dist)\n",
    "    start_word = id_to_vocab[next_ind]\n",
    "    sentence += \" \" + start_word\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### start word: 'the'\n",
    "\n",
    "* the nations shall many women working on the people when israel which are known loser who hath sent benaiah the white as i hope of macedonia and invent to the appearance they not nor taking over theaters this is mccarthyism\n",
    "* the original costume was susan berry\n",
    "* the midst of seventy years agothis is mad sometimes referred to be long massive tax just named eutychus being stubborn can call it\n",
    "* the bar them that thou with his works nay but you have sinned against the lord\n",
    "\n",
    "### start word: 'who'\n",
    "* who escaped alone and treasuries shelemiah shemariah and moab and this country is the one came up he defrauded us is through phenice and thou shalt not hearken unto aaron the obamacare\n",
    "* who was a son of them thou shalt thou art not the word again despite obamas terrible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x43714 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_norm.getrow(37577)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
